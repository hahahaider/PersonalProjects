{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16be466",
   "metadata": {},
   "source": [
    "#### Feed Forward Neural Network Mathematics\n",
    "\n",
    "The following is a programme that uses the neural network structure on the MNIST dataset in order to classify handwritten numbers. \n",
    "Although there are several python libraries that have in-built neural network functions, to aid in my understanding of the technique, I have coded all elements of a NN to run the MNIST classifier.\n",
    "\n",
    "Below is some of the mathematics behind NN to aid in the explaination of what each function does. The reason for the success of NN in approximating non-linear functions lies in the proof by George Cybenko of the Universal Approximation Theorem.\n",
    "\n",
    "##### Mathematics behind NN for Matrix Inputs\n",
    "\n",
    "The following is the set up for a feed forward neural network that uses matrix inputs. \n",
    "\n",
    "- number of layers: $L$\n",
    "- width of layers: $\\eta_0,...,\\eta_L$\n",
    "- input layer: $X^{(0)} \\in \\mathbb{R}^{\\eta_0 \\times n}$; $n$ observations stacked in columns\n",
    "- other layers: $X^{(l)} \\in \\mathbb{R}^{\\eta_{l}\\times n}$, where $X^{(l)} = \\sigma^{(l)}\\left(A^{(l)}X^{(l-1)} + B^{(l)}\\right)$\n",
    "- weight matrix: $A^{(l)} \\in \\mathbb{R}^{\\eta_l \\times \\eta_{l-1}}$; the rows of $A^{(l)}$ are the weight vectors corresponding to the $\\eta_l$ nodes in the $l$-th layer\n",
    "\n",
    "We will aim to walkthrough the mathematics of the NN by an explaination of the forward and backpropogation processes as well as the gradient descent optimisation process through an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe60bce",
   "metadata": {},
   "source": [
    "##### Example: 3 Layer Neural Network Matrix Input\n",
    "\n",
    "Forward Propagation: \n",
    "1. $X^{(0)} \\in \\mathbb{R}^{\\eta_0 \\times n}$\n",
    "\n",
    "2. $X^{(1)} = \\sigma^{(1)}\\left(A^{(1)}X^{(0)} + B^{(1)}\\right)$\n",
    "\n",
    "    $\\sigma^{(1)} := $ Piecewise ReLU (Rectified Linear Unit),\n",
    "\n",
    "    $\\sigma^{(1)}(z_{ij}) = \n",
    "    \\begin{cases}\n",
    "    z_{ij} & \\text{if } z_{ij} >0 \\\\\n",
    "    0 & \\text{if } z_{ij} \\leq 0 \n",
    "    \\end{cases}$\n",
    "\n",
    "3. $X^{(2)} = \\sigma^{(2)}\\left(A^{(2)}X^{(1)} + B^{(2)}\\right)$\n",
    "\n",
    "    $\\sigma^{(2)} := $ Piecewise Softmax,\n",
    "\n",
    "    $\\sigma^{(2)}(z_{ij}) = \\frac{e^{z_{ij}}}{\\sum^{\\eta_2}_{k = 1}e^{z_{kj}}}$\n",
    "\n",
    "Backpropagation:\n",
    "\n",
    "The process of backpropagation enables us to optimise the NN on the trainable parameters, the weight and biases for each layer of the NN. This begins by quantifying the misaccuracy of our classification NN. We use a loss function for which we then use chain-rule to find the gradient of the loss function with respect to our trainable parameters. We can then carry out gradient descent. \n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "$A^{(l)} = A^{(l)} - \\alpha \\frac{\\partial L}{\\partial A^{(l)}}$\n",
    "\n",
    "$B^{(l)} = B^{(l)} - \\alpha \\frac{\\partial L}{\\partial B^{(l)}}$\n",
    "\n",
    "To calculate the derivative of the loss function with respect to the weights and biases, we use the chain rule. The steps are as follows:\n",
    "\n",
    "1. Loss function: Cross entropy loss\n",
    "\n",
    "    $L(X,Y) = -\\frac{1}{n}\\sum_{j=1}^{n}\\sum_{i=1}^{\\eta_2}y_{ij}\\cdot log(x_{ij})$\n",
    "\n",
    "2. Chain rule steps:\n",
    "\n",
    "    $\\frac{\\partial L}{\\partial A^{(2)}} = \\frac{\\partial{L}}{\\partial X^{(2)}} \\cdot \\frac{\\partial X^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial A^{(2)}}$\n",
    "\n",
    "    $\\frac{\\partial L}{\\partial x^{(2)}}_{lk} = \\frac{\\partial L}{\\partial x_{lk}} = -\\frac{1}{n}\\sum_{j=1}^{n}\\sum_{i=1}^{\\eta_2}y_{ij}\\cdot \\frac{\\partial log(x_{ij})}{\\partial x_{lk}} = -\\frac{1}{n}\\sum_{j=1}^{n}\\sum_{i=1}^{\\eta_2}\\frac{y_{ij}}{x_{ij}}$\n",
    "\n",
    "    $\\frac{\\partial X^{(2)}_{ij}}{\\partial Z^{(2)}_{lk}} = \\frac{\\partial}{\\partial z^{(2)}_{lk}} \\frac{e^{z^{(2)}_{ij}}}{\\sum_m e^{z^{(2)}_{mj}}} = x^{(2)}_{ij} \\cdot \\frac{\\partial}{\\partial z^{(2)}_{lk}} log(x^{(2)}_{ij})$\n",
    "\n",
    "    \n",
    "    $\\frac{\\partial}{\\partial z^{(2)}_{lk}}log(x^{(2)}_{ij}) = \\frac{\\partial z^{(2)}_{ij}}{\\partial z^{(2)}_{lk}} - \\frac{\\partial}{\\partial z^{(2)}_{lk}}log(\\sum_m e^{z^{(2)}_{mj}}) = \\frac{\\partial z^{(2)}_{ij}}{\\partial z^{(2)}_{lk}} - \\left[\\frac{1}{\\sum_m e^{z^{(2)}_{mj}}}\\sum_m \\frac{\\partial e^{z^{(2)}_{mj}}}{\\partial z^{(2)}_{lk}}\\right] = \\frac{\\partial z^{(2)}_{ij}}{\\partial z^{(2)}_{lk}} - \\left[\\frac{1}{\\sum_m e^{z^{(2)}_{mj}}}\\sum_m e^{z^{(2)}_{mj}}\\frac{\\partial z^{(2)}_{mj}}{\\partial z^{(2)}_{lk}}\\right]$\n",
    "\n",
    "    $\\frac{\\partial X^{(2)}_{ij}}{\\partial Z^{(2)}_{lk}} = \n",
    "    \\begin{cases}\n",
    "    x^{(2)}_{ij}(1 - x^{(2)}_{ij}) & \\text{if } l = i, k = j\\\\\n",
    "    -x^{(2)}_{ij}x^{(2)}_{lj} & \\text{if } l \\neq i, k = j \\\\\n",
    "    0 & \\text{if } k \\neq j\n",
    "    \\end{cases}$\n",
    "\n",
    "    $\\frac{\\partial L}{\\partial z^{(2)}_{lk}} = -\\frac{1}{n}\\sum_{j=1}^{n}\\sum_{i=1}^{\\eta_2}(\\frac{y_{ij}}{x_{ij}} \\cdot \\frac{\\partial x^{(2)}_{ij}}{\\partial z^{(2)}_{lk}})$, but $\\frac{\\partial x^{(2)}_{ij}}{\\partial z^{(2)}_{lk}}$ only exists when $k = j$ therefore the sum $\\sum_ j^n$ is made redundant\n",
    "\n",
    "    $\\implies \\frac{\\partial L}{\\partial Z^{(2)}_{lj}} = -\\frac{1}{n}(y_{lj}(1-x^{(2)}_{lj}) + \\sum_{i\\neq l}-y_{ij}x^{(2)}_{lj}) = -\\frac{1}{n}(y_{lj} + \\sum_{i}-y_{ij}x^{(2)}_{lj}) = -\\frac{1}{n}(y_{lj} + x^{(2)}_{lj}\\sum_{i}-y_{ij})$\n",
    "\n",
    "    $\\implies \\frac{\\partial L}{\\partial Z^{(2)}_{lj}} = -\\frac{1}{n}(y_{lj} - x^{(2)}_{lj}) = \\frac{1}{n}(x^{(2)}_{lj} - y_{lj})$\n",
    "\n",
    "    $\\implies \\frac{\\partial L}{\\partial Z^{(2)}} = \\frac{1}{n}(X^{(2)}-Y)$\n",
    "\n",
    "    $\\frac{\\partial Z^{(2)}}{\\partial A^{(2)}} = \\frac{\\partial}{\\partial A^{(2)}}(A^{(2)}X^{(1)} + B^{(2)}) = X^{(1)T}$\n",
    "\n",
    "    $\\frac{\\partial L}{\\partial A^{(2)}} = \\frac{\\partial{L}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial A^{(2)}} = \\frac{1}{n}(X^{(2)}-Y)X^{(1)T}$\n",
    "\n",
    "    For the bias term, we have:\n",
    "\n",
    "    $\\frac{\\partial L}{\\partial B^{(2)}} = \\frac{\\partial L}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial B^{(2)}}$, where $\\frac{\\partial Z^{(2)}}{\\partial B^{(2)}} = I \\in \\mathbb{R}^{\\eta_2 \\times n}$\n",
    "\n",
    "    However, as $B^{(2)}$ is actually a column vector broadcasted to fit the dimensions, we sum across the columns for the gradient of the bias term.\n",
    "\n",
    "    $\\implies \\frac{\\partial L}{\\partial B^{(2)}} = \\frac{\\partial L}{\\partial Z^{(2)}} = \\frac{1}{n}\\sum_j(X^{(2)} - Y)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7b609",
   "metadata": {},
   "source": [
    "For the tunable parameters of the first layer, gradient descent requires the following values:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial A^{(1)}} = \\frac{\\partial{L}}{\\partial X^{(2)}} \\cdot \\frac{\\partial X^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial X^{(1)}} \\cdot \\frac{\\partial X^{(1)}}{\\partial Z^{(1)}} \\cdot  \\frac{\\partial Z^{(1)}}{\\partial A^{(1)}}$;\n",
    "\n",
    "for the bias term:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B^{(1)}} = \\frac{\\partial{L}}{\\partial X^{(2)}} \\cdot \\frac{\\partial X^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial X^{(1)}} \\cdot \\frac{\\partial X^{(1)}}{\\partial Z^{(1)}} \\cdot  \\frac{\\partial Z^{(1)}}{\\partial B^{(1)}}$\n",
    "\n",
    "We have: \n",
    "\n",
    "$\\frac{\\partial Z^{(2)}}{\\partial X^{(1)}} = \\frac{\\partial}{\\partial X^{(1)}}(A^{(2)}X^{(1)}+B^{(2)}) = A^{(2)}$\n",
    "\n",
    "$\\frac{\\partial X^{(1)}}{\\partial Z^{(1)}}_{ij} = \\sigma^{(1)'}(Z^{(1)}) = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } z_{ij} > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "$\\frac{\\partial Z^{(1)}}{\\partial A^{(1)}} = X^{(0)T}$\n",
    "\n",
    "$\\implies \\frac{\\partial L}{\\partial A^{(1)}} = \\frac{1}{n}A^{(2)T}\\cdot(X^{(2)}-Y) \\odot \\sigma^{(1)'}(Z^{(1)}) \\cdot X^{(0)T}$\n",
    "\n",
    "Similarly, for the bias term, we have: \n",
    "\n",
    "$\\frac{\\partial Z^{(1)}}{\\partial B^{(1)}} = I \\in \\mathbb{R}^{\\eta_1 \\times n}$, where again, the matrix is a column vector broadcasted to fit the dimension.\n",
    "\n",
    "$\\implies \\frac{\\partial L}{\\partial B^{(1)}} = \\frac{1}{n}\\sum_j(A^{(2)T}\\cdot(X^{(2)}-Y) \\odot \\sigma^{(1)'}(Z^{(1)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cc047",
   "metadata": {},
   "source": [
    "Therfore, we have the following for the update steps as part of the gradient descent processes:\n",
    "\n",
    "$A^{(2)} = A^{(2)} - \\alpha \\frac{\\partial L}{\\partial A^{(2)}}$, where $\\frac{\\partial L}{\\partial A^{(2)}} = \\frac{1}{n}(X^{(2)}-Y)X^{(1)T}$\n",
    "\n",
    "$B^{(2)} = B^{(2)} - \\alpha\\frac{\\partial L}{\\partial B^{(2)}}$, where $\\frac{\\partial L}{\\partial B^{(2)}} = \\frac{1}{n}\\sum_j(X^{(2)}-Y)$\n",
    "\n",
    "$A^{(1)} = A^{(1)} - \\alpha \\frac{\\partial L}{\\partial A^{(1)}}$, where $\\frac{\\partial L}{\\partial A^{(1)}} = \\frac{1}{n}A^{(2)T}\\cdot(X^{(2)}-Y) \\odot \\sigma^{(1)'}(Z^{(1)}) \\cdot X^{(0)T}$\n",
    "\n",
    "$B^{(1)} = B^{(1)} - \\alpha\\frac{\\partial L}{\\partial B^{(1)}}$, where $\\frac{\\partial L}{\\partial B^{(1)}} = \\frac{1}{n}\\sum_j(A^{(2)T}\\cdot(X^{(2)}-Y) \\odot \\sigma^{(1)'}(Z^{(1)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "292f2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6fa3b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b9fe6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f0cbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "n,p = data.shape                    # we have n = 42000 samples and p = 784 pixels per image\n",
    "np.random.shuffle(data)             # shuffle to initialise the data\n",
    "data_dev = data[0:1000].T           # after transposing the first 1000 rows of the dataset,\n",
    "                                    # the rows become the pixels and the columns are samples\n",
    "Y_dev = data_dev[0]                 # identify the labels from the data\n",
    "X_dev = data_dev[1:p]               # identify the pixel data\n",
    "\n",
    "data_train = data[1000:n].T         # the rest of the data is to train the NN on \n",
    "Y_train = data_train[0]             # we do the same\n",
    "X_train = data_train[1:p]           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b2ba674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    A1 = np.random.randn(10, 784) * np.sqrt(2 / 784)  # He initialization\n",
    "    B1 = np.zeros((10, 1))\n",
    "    A2 = np.random.randn(10, 10) * np.sqrt(2 / 10)\n",
    "    B2 = np.zeros((10, 1))\n",
    "    return A1, B1, A2, B2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)                                 # Element-wise comparison to 0\n",
    "\n",
    "def LeReLU(Z):\n",
    "    return np.maximum(0.01 * Z, Z)                          # Leaky ReLU to prevent dead neurons\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=0, keepdims=True)                # Numerical stability\n",
    "    e_x = np.exp(x)  \n",
    "    return e_x / (e_x.sum(axis=0, keepdims=True) + 1e-8)    # np.exp is elementwise exp, and np.sum sums across the axis = 0 means column sum\n",
    "\n",
    "def forward_prop(A1,B1,A2,B2,X0):                       \n",
    "    Z1 = A1.dot(X0) + B1                                    # numpy broadcasts the shape of the vector to match the matrix.\n",
    "    X1 = LeReLU(Z1)\n",
    "    Z2 = A2.dot(X1) + B2\n",
    "    X2 = softmax(Z2)\n",
    "    return Z1,X1,Z2,X2\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.max()+1,Y.size))                # create a grid of zeros, rows = classes, columns = samples (1000)\n",
    "    one_hot_Y[Y, np.arange(Y.size)] = 1                     # np.arange(Y.size) creates an array of entries from 0 to 1000, paired with the label, \n",
    "                                                            # it puts a 1 in the correct row and increments the column\n",
    "    return one_hot_Y\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def deriv_LeReLU(Z):\n",
    "    return np.where(Z > 0, 1, 0.01)                         # Derivative of Leaky ReLU\n",
    "\n",
    "def back_prop(Z1,X1,Z2,X2,A2,X0,Y):\n",
    "    n_train = Y.size\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = X2 - one_hot_Y\n",
    "    dA2 = 1 / n_train * dZ2.dot(X1.T)\n",
    "    dB2 = 1/ n_train * np.sum(dZ2, axis = 1, keepdims= True)                          \n",
    "    dZ1 = A2.T.dot(dZ2) * deriv_LeReLU(Z1)\n",
    "    dA1 = 1 / n_train * dZ1.dot(X0.T)\n",
    "    dB1 = 1/ n_train * np.sum(dZ1,axis=1,keepdims=True)\n",
    "    return dA1,dB1,dA2,dB2\n",
    "\n",
    "def update_params(A1,B1,A2,B2,dA1,dB1,dA2,dB2,alpha):\n",
    "    A1 = A1 - alpha*dA1\n",
    "    B1 = B1 - alpha*dB1\n",
    "    A2 = A2 - alpha*dA2\n",
    "    B2 = B2 - alpha*dB2\n",
    "    return A1,B1,A2,B2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83b1c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "[[-4.48531617]\n",
      " [-5.45209726]\n",
      " [-4.47175194]\n",
      " [-6.43592812]\n",
      " [-4.74077038]\n",
      " [-4.07740524]\n",
      " [-4.93070581]\n",
      " [-5.15654028]\n",
      " [-4.82461889]\n",
      " [-5.04494842]]\n"
     ]
    }
   ],
   "source": [
    "B1 = np.random.rand(10,1) - 0.5\n",
    "print(B1.shape)\n",
    "one_hot_Y = one_hot(Y_dev)\n",
    "\n",
    "B1 = B1 - 0.05*(np.sum(one_hot_Y,axis=1).reshape(10,1)) \n",
    "\n",
    "print(B1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8a5cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X2):\n",
    "    return np.argmax(X2,0)\n",
    "\n",
    "def get_accuracy(predictions,Y):\n",
    "    print(predictions,Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X0, Y, iterations, alpha, decay_rate=0.99):\n",
    "    A1, B1, A2, B2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, X1, Z2, X2 = forward_prop(A1, B1, A2, B2, X0)\n",
    "        dA1, dB1, dA2, dB2 = back_prop(Z1, X1, Z2, X2, A2, X0, Y)\n",
    "        A1, B1, A2, B2 = update_params(A1, B1, A2, B2, dA1, dB1, dA2, dB2, alpha)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            alpha *= decay_rate  # Decay learning rate\n",
    "            print(f\"Iteration {i}, Accuracy: {get_accuracy(get_predictions(X2), Y)}, Alpha: {alpha}\")\n",
    "\n",
    "    return A1, B1, A2, B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dea382b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 5 ... 2 8 2] [9 9 5 ... 1 0 9]\n",
      "Iteration 0, Accuracy: 0.08275609756097561, Alpha: 0.0099\n",
      "[4 8 8 ... 1 0 8] [9 9 5 ... 1 0 9]\n",
      "Iteration 50, Accuracy: 0.3333170731707317, Alpha: 0.009801\n",
      "[4 7 3 ... 1 0 7] [9 9 5 ... 1 0 9]\n",
      "Iteration 100, Accuracy: 0.5303170731707317, Alpha: 0.00970299\n",
      "[4 7 5 ... 1 0 7] [9 9 5 ... 1 0 9]\n",
      "Iteration 150, Accuracy: 0.5310243902439025, Alpha: 0.0096059601\n",
      "[4 7 3 ... 1 0 9] [9 9 5 ... 1 0 9]\n",
      "Iteration 200, Accuracy: 0.6392682926829268, Alpha: 0.009509900499\n",
      "[4 9 5 ... 1 0 9] [9 9 5 ... 1 0 9]\n",
      "Iteration 250, Accuracy: 0.6614390243902439, Alpha: 0.00941480149401\n",
      "[4 9 5 ... 1 0 9] [9 9 5 ... 1 0 9]\n",
      "Iteration 300, Accuracy: 0.7350487804878049, Alpha: 0.0093206534790699\n",
      "[4 7 5 ... 1 0 9] [9 9 5 ... 1 0 9]\n",
      "Iteration 350, Accuracy: 0.7066097560975609, Alpha: 0.0092274469442792\n",
      "[4 9 5 ... 1 0 9] [9 9 5 ... 1 0 9]\n",
      "Iteration 400, Accuracy: 0.7597317073170732, Alpha: 0.009135172474836408\n",
      "[4 9 3 ... 1 0 9] [9 9 5 ... 1 0 9]\n",
      "Iteration 450, Accuracy: 0.8349024390243902, Alpha: 0.009043820750088045\n"
     ]
    }
   ],
   "source": [
    "A1,B1,A2,B2 = gradient_descent(X_train,Y_train, 500, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
